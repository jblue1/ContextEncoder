"""
Trains a context-encoder network to perform image inpainting as described in 'Context Encoders: Feature Learning by
Inpainting' by Pathak et al. Note that 'generator' and 'autoencoder' are the same thing and the terms used
interchangeably.

Notes:
- Data format of images changed depending on whether or not a gpu is being used. For an explanation, see 'Data Formats'
in https://www.tensorflow.org/guide/performance/overview
- The region extracted from the original image fell was -10 < x < 127.5, -117.5 < y < 20. Since the original x and y
limits were (-275, 275), to extract that region from a 128x128 numpy array arr uses the slicing arr[60:92, 61:93]
"""

import tensorflow as tf
from ContextEncoder import CE_model
import load_data
import os
import matplotlib.pyplot as plt
import time
import click
from datetime import date
import pandas as pd

# Build models, define losses and optimizers
cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,
                                                   from_logits=True)
MSE = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)


def discriminator_loss(real_output, fake_output):
    """
    Defines the discriminator loss as the sum of
    1. Binary cross entropy of its predictions of real centers against a tensor of all ones
    2. Binary cross entropy of its predictions of generated centers against a tensor of all zeros

    real_output: tensor of shape (batch_size, 1, 1, 1) - the discriminators predictions against real centers
    fake_output: tensor of shape (batch_size, 1, 1, 1) - the discriminators predictions against centers generated by the
    generator
    """

    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss


def generator_loss(fake_output, y_true, y_pred, overlap, use_gpu, weight_l2=0.9, weight_adv=0.1, use_adv=True):
    """
    Defines the generator loss as the weighted sum of
    1. Binary cross entropy of the discriminators predictions against a tensor of all ones (i.e. how well the generator
    is tricking the discriminator.
    2. Mean Squared error between the generated center and the real center


    fake_output: tensor of shape (batch_size, 1, 1, 1) - the discriminators predictions against centers generated by the
    generator
    y_true: tensor - real center image array
    y_pred: tensor - center image array generated by the autoencoder
    overlap: int - specifies the number of pixels to overlap the outside image with the center. See Sec 5.1 of Pathak
    et al.
    """
    adv_loss = cross_entropy(tf.ones_like(fake_output), fake_output)
    # isolate overlapped areas of the image
    if overlap != 0:
        #  when using gpu channels are first
        if use_gpu:
            y_true_center = y_true[:, :, overlap:-overlap, overlap:-overlap]
            y_pred_center = y_pred[:, :, overlap:-overlap, overlap:-overlap]
            y_true_overlap_left = y_true[:, :, :overlap, :overlap]
            y_true_overlap_right = y_true[:, :, -overlap:, -overlap:]
            y_pred_overlap_left = y_pred[:, :, :overlap, :overlap]
            y_pred_overlap_right = y_pred[:, :, -overlap:, -overlap:]
        # when using cpu channels are last
        else:
            y_true_center = y_true[:, overlap:-overlap, overlap:-overlap, :]
            y_pred_center = y_pred[:, overlap:-overlap, overlap:-overlap, :]
            y_true_overlap_left = y_true[:, :overlap, :overlap, :]
            y_true_overlap_right = y_true[:, -overlap:, -overlap:, :]
            y_pred_overlap_left = y_pred[:, :overlap, :overlap, :]
            y_pred_overlap_right = y_pred[:, -overlap:, -overlap:, :]

        center_loss = MSE(y_true_center, y_pred_center)
        # want overlap loss to be 10x higher than the picture
        overlap_loss = 10 * (
                MSE(y_true_overlap_left, y_pred_overlap_left) + MSE(y_true_overlap_right, y_pred_overlap_right))
        l2_loss = center_loss + overlap_loss
    else:

        l2_loss = MSE(y_true, y_pred)
    if use_adv:
        total_loss = weight_l2 * l2_loss + weight_adv * adv_loss
    else:
        total_loss = l2_loss
    return total_loss


@tf.function
def take_step(image_contexts, images, missing_region, overlap, generator, discriminator, use_gpu, generator_optimizer,
              discriminator_optimizer):
    """
    One forward step and, if training=True, one pass of backpropegation for both the generator and discriminator.
    The tf.function decorator means that this function is 'compiled' into a tensorflow graph a la tensorflow 1.x
    to increase speed.

    images: tensor - images to train with
    real_centers: tensor - real image centers
    """
    # 'fDx' in paper, train the discriminator
    with tf.GradientTape() as disc_tape:
        real_output = discriminator(missing_region, training=True)
        generated_images = generator(image_contexts, training=False)
        if use_gpu:
            fake_output = discriminator(generated_images[:, :, 60:92, 61:93], training=True)
        else:
            fake_output = discriminator(generated_images[:, 60:92, 61:93, :], training=True)
        disc_loss = discriminator_loss(real_output, fake_output)

    discriminator_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))

    # 'fGx' in paper, train the generator
    with tf.GradientTape() as gen_tape:
        generated_images = generator(image_contexts, training=True)
        gen_loss = generator_loss(fake_output, images, generated_images, overlap, use_gpu)

    generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)
    generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))

    return gen_loss, disc_loss


@tf.function
def calc_losses(image_contexts, images, missing_region, overlap, use_gpu, generator, discriminator):
    """
    Calculates losses without training
    """
    generated_images = generator(image_contexts, training=False)
    real_output = discriminator(missing_region, training=False)
    if use_gpu:
        fake_output = discriminator(generated_images[:, :, 60:92, 61:93], training=True)
    else:
        fake_output = discriminator(generated_images[:, 60:92, 61:93, :], training=True)

    disc_loss = discriminator_loss(real_output, fake_output)
    gen_loss = generator_loss(fake_output, images, generated_images, overlap, use_gpu)
    l2_gen_loss = generator_loss(fake_output, images, generated_images, overlap, use_gpu, use_adv=False)

    return gen_loss, disc_loss, l2_gen_loss


def plot_loss(train_gen_loss, val_gen_loss, train_disc_loss, val_disc_loss, save_dir):
    """
    Saves a plot of the various losses over time. Takes in lists of the losses, and the directory
    to save the plot in.
    """
    fig, (ax1, ax2) = plt.subplots(2, 1)
    ax1.plot(train_gen_loss, color='r', label='Training Loss')
    ax1.plot(val_gen_loss, color='b', label='Validation Loss')
    ax1.set_xlabel('Num. Epochs')
    ax1.set_ylabel('Loss')
    ax1.set_title('Generator Loss')
    ax1.legend()

    ax2.plot(train_disc_loss, color='r', label='Training Loss')
    ax2.plot(val_disc_loss, color='b', label='Validation Loss')
    ax2.set_xlabel('Num. Epochs')
    ax2.set_ylabel('Loss')
    ax2.set_title('Discriminator Loss')
    ax2.legend()

    fig.subplots_adjust(hspace=.5)
    filename = os.path.join(save_dir, 'Loss_history.png')
    plt.savefig(filename)
    plt.close()


def save_pictures(image_context_batch, missing_region_batch, epoch, use_gpu, save_dir, generator, prefix='',
                  num_pictures=5):
    """
    Saves a set of images, where each image contains the real broken spiral and the broken
    spiral generated by the model for comparison.

    image_batch: tensor - batch of input images
    center_batch: tensor - batch of broken spiral images
    epoch: int - current epoch
    save_dir: path - the directory to save the pictures in
    prefix: string - used to distinguish training and validation images in the filename
    num_pictures: int - number of images to save
    """
    # take data from [-1, 1] range to [0, 1] range for plotting
    gen_images_batch = (generator(image_context_batch, training=False) + 1) / 2

    if len(image_context_batch) < num_pictures:
        num_pictures = 1

    if use_gpu:
        image_context_batch = tf.transpose(image_context_batch, (0, 2, 3, 1))
        gen_images_batch = tf.transpose(gen_images_batch, (0, 2, 3, 1))

    gen_missing_region_batch = gen_images_batch[:, 60:92, 61:93, :]
    for i in range(num_pictures):
        filename = os.path.join(save_dir, prefix + '_epoch_{}_{}'.format(epoch, i) + '.png')
        fig, axs = plt.subplots(2, 2)
        fig.suptitle('Epoch: {}'.format(epoch))
        axs[0, 0].imshow(image_context_batch[i, :, :, :])
        axs[0, 0].set_title('Image Context')
        axs[0, 1].imshow(gen_images_batch[i, :, :, :])
        axs[0, 1].set_title('Generated Image')
        axs[1, 0].imshow(missing_region_batch[i, :, :, :])
        axs[1, 0].set_title('Missing Region')
        axs[1, 1].imshow(gen_missing_region_batch[i, :, :, :])
        axs[1, 1].set_title('Generated Missing Region')
        fig.subplots_adjust(hspace=.3)
        plt.savefig(filename)
        plt.close()


def train(train_dataset, val_dataset, epochs, overlap, use_gpu, lr, save_dir):
    """
    Trains model, saves a model checkpoint every 5 epochs, and plots a graph of training and validation loss for both
    the autoencoder and discriminator after training.
    """
    generator = CE_model.build_autoencoder(use_gpu)
    discriminator = CE_model.build_discriminator(use_gpu)
    generator_optimizer = tf.keras.optimizers.Adam(lr * 10)
    discriminator_optimizer = tf.keras.optimizers.Adam(lr)
    checkpoint_dir = os.path.join(save_dir, 'training_checkpoints/')
    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                     discriminator_optimizer=discriminator_optimizer,
                                     generator=generator,
                                     discriminator=discriminator)
    # save pictures using model in its initialized state for comparison
    image_context, image = next(iter(train_dataset))
    missing_region = image[:, 60:92, 61:93, :]
    if use_gpu:
        image_context = tf.transpose(image, (0, 3, 1, 2))
        missing_region = tf.transpose(missing_region, (0, 3, 1, 2))
    save_pictures(image_context[0:1, :, :, :], missing_region[0:1, :, :, :], 0, use_gpu, save_dir, generator,
                  prefix='init')

    list_train_gen_loss = []
    list_train_disc_loss = []
    list_val_gen_loss = []
    list_val_disc_loss = []
    list_l2_val_gen_loss = []
    for epoch in range(epochs):
        start = time.time()
        train_gen_loss = 0
        train_disc_loss = 0
        val_gen_loss = 0
        val_disc_loss = 0
        val_l2_gen_loss = 0
        count_train = 0
        count_val = 0
        for image_context_batch, image_batch in train_dataset:
            missing_region_batch = image_batch[:, 60:92, 61:93, :]
            # if using gpu need to transpose tensor to channels first
            if use_gpu:
                image_context_batch = tf.transpose(image_context_batch, (0, 3, 1, 2))
                image_batch = tf.transpose(image_batch, (0, 3, 1, 2))
                missing_region_batch = tf.transpose(missing_region_batch, (0, 3, 1, 2))
            gen_loss, disc_loss = take_step(image_context_batch,
                                            image_batch,
                                            missing_region_batch,
                                            overlap,
                                            generator,
                                            discriminator,
                                            use_gpu,
                                            generator_optimizer,
                                            discriminator_optimizer)
            train_gen_loss += gen_loss
            train_disc_loss += disc_loss
            count_train += 1
        # every 5th epoch (and the first) save training images for comparison
        if (epoch + 1) % 5 == 0 or epoch == 0:
            save_pictures(image_context_batch, missing_region_batch, epoch + 1, use_gpu, save_dir, generator,
                          prefix='train')
        for image_context_batch, image_batch in val_dataset:
            missing_region_batch = image_batch[:, 60:92, 61:93, :]
            if use_gpu:
                image_context_batch = tf.transpose(image_context_batch, (0, 3, 1, 2))
                image_batch = tf.transpose(image_batch, (0, 3, 1, 2))
                missing_region_batch = tf.transpose(missing_region_batch, (0, 3, 1, 2))
            gen_loss, disc_loss, l2_gen_loss = calc_losses(image_context_batch, image_batch, missing_region_batch,
                                                           overlap, use_gpu, generator, discriminator)
            val_gen_loss += gen_loss
            val_disc_loss += disc_loss
            val_l2_gen_loss += l2_gen_loss
            count_val += 1

        # every 5th epoch (and the first) save validation images for comparison and save model checkpoints
        if (epoch + 1) % 5 == 0 or epoch == 0:
            checkpoint.save(file_prefix=checkpoint_prefix)
            save_pictures(image_context_batch, missing_region_batch, epoch + 1, use_gpu, save_dir, generator,
                          prefix='val')

        # normalize loss by size of dataset
        train_gen_loss = train_gen_loss / count_train
        train_disc_loss = train_disc_loss / count_train
        val_gen_loss = val_gen_loss / count_val
        val_disc_loss = val_disc_loss / count_val
        val_l2_gen_loss = val_l2_gen_loss / count_val

        # .numpy() converts the tensors to np arrays, which in this case results in just floats.
        list_train_gen_loss.append(train_gen_loss.numpy())
        list_train_disc_loss.append(train_disc_loss.numpy())
        list_val_gen_loss.append(val_gen_loss.numpy())
        list_val_disc_loss.append(val_disc_loss.numpy())
        list_l2_val_gen_loss.append(val_l2_gen_loss.numpy())

        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))
        print('Generator - Training loss {} --- Validation loss {}'.format(train_gen_loss, val_gen_loss))
        print('Discriminator - Training loss {} --- Validation loss {} \n'.format(train_disc_loss, val_disc_loss))

    loss_df = pd.DataFrame({'Gen Training Loss': pd.Series(list_train_gen_loss),
                            'Gen Val Loss': pd.Series(list_val_gen_loss),
                            'Disc Training Loss': pd.Series(list_train_disc_loss),
                            'Disc Val Loss': pd.Series(list_val_disc_loss),
                            'Gen L2 Val Loss': pd.Series(list_l2_val_gen_loss)})
    plot_loss(list_train_gen_loss, list_val_gen_loss, list_train_disc_loss, list_val_disc_loss, save_dir)

    filename = os.path.join(save_dir, 'losses.csv')
    loss_df.to_csv(filename)


def write_info_file(save_dir, data_path, overlap, batch_size, use_gpu, epochs, lr,
                    run_number):
    """
    Writes a text file to the save directory with a summary of the hyper-parameters used for training
    """
    filename = os.path.join(save_dir, 'run_info.txt')
    info_list = ['ContextEncoder Hyper-parameters: Run {} \n'.format(run_number),
                 'Training data found at: {} \n'.format(data_path),
                 'Overlap: {} \n'.format(overlap),
                 'Batch Size: {} \n'.format(batch_size),
                 'Use GPU: {} \n'.format(use_gpu),
                 'Epochs: {} \n'.format(epochs),
                 'Learning Rate: {} \n'.format(lr)]

    with open(filename, 'w') as f:
        f.writelines(info_list)


@click.command()
@click.argument('data_path', type=click.Path(exists=True, readable=True))
@click.option('--overlap', default=7, help='Size of overlap (in pixels) of the predicted image with the real image')
@click.option('--batch_size', default=64)
@click.option('--use_gpu/--no_gpu', default=False)
@click.option('--epochs', default=50)
@click.option('--lr', default=2e-4, help='Learning rate for Adam optimizer')
@click.option('--run_number', default=1, help='ith run of the day')
def main(data_path, overlap, batch_size, use_gpu, epochs, lr, run_number):
    # construct directory to store images, model checkpoints etc.
    today = str(date.today())
    run_number = '_' + str(run_number)
    save_dir = './Run_' + today + run_number

    if os.path.exists(save_dir):
        ans = input(
            'The directory this run will write to already exists, would you like to overwrite it? ([y/n])')
        if ans == 'y':
            pass
        else:
            return
    else:
        os.makedirs(save_dir)
    write_info_file(save_dir, data_path, overlap, batch_size, use_gpu, epochs, lr,
                    run_number)

    filename = os.path.join(data_path, 'batch_1_images.h5')
    train_dataset, val_dataset = load_data.load_simulated_data(filename)
    for i in range(4):
        filename = os.path.join(data_path, 'batch_{}_images.h5'.format(i + 1))
        new_train_ds, new_val_ds = load_data.load_simulated_data(filename)
        train_dataset.concatenate(new_train_ds)
        val_dataset.concatenate(new_val_ds)
    train_dataset = train_dataset.batch(batch_size)
    val_dataset = val_dataset.batch(batch_size)

    train(train_dataset, val_dataset, epochs, overlap, use_gpu, lr, save_dir)


if __name__ == '__main__':
    main()
